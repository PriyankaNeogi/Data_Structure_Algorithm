{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1be34013",
   "metadata": {},
   "source": [
    "## Logarithmic Time Complexity - O(log n) :\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a2b3f",
   "metadata": {},
   "source": [
    "Logarithmic time complexity, or O(log N), \n",
    "- is central to algorithms like binary search.\n",
    "  \n",
    "Suppose we have a list of eight elements:  \n",
    "\n",
    "1, 2, 3, 4, 5, 6, 7, 8  \n",
    "\n",
    "The task is to search for a target value, say `6`, \n",
    "- and return its index. \n",
    "- Linear search, which involves checking each element one by one. \n",
    "\n",
    "For example, \n",
    "- you start at the first element, \n",
    "- check if it’s `6`, and if not, move to the next one, \n",
    "- repeating this process until you find the target. \n",
    "    - This approach has a time complexity of O(N), \n",
    "    - as you might need to scan the entire list.  \n",
    "\n",
    "Now imagine the list size is one billion elements.\n",
    "- A linear search would require up to one billion steps. \n",
    "- While linear search is straight forward and effective for smaller datasets, \n",
    "- it becomes inefficient with very large datasets. \n",
    "\n",
    "This is where **binary search**, \n",
    "- which operates with a time complexity of O(log N), \n",
    "- becomes a game changer.  \n",
    "\n",
    "For a list of eight elements,\n",
    "- a binary search can locate the target in just three steps.\n",
    "\n",
    "- Similarly, for a dataset of one billion elements, \n",
    "    - a binary search would require only 31 steps. \n",
    "    - This massive efficiency improvement is the strength of logarithmic time complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6129f7",
   "metadata": {},
   "source": [
    "#### Binary search algorithm - class 16."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b5cd62",
   "metadata": {},
   "source": [
    "Logarithmic complexity can be expressed mathematically.\n",
    "\n",
    "For example, given a data size, \n",
    "- the equation \\( 2^n = \\{data size} \\) determines the number of splits, \\( n \\).\n",
    "- By taking the logarithm, this relationship becomes \\( n = \\log_2(\\{data size}) \\),\n",
    "    - meaning the required splits are proportional to the base-2 logarithm of the data size.\n",
    "\n",
    "For instance, if the data size is 16, the question is: \"What power of 2 equals 16?\" \n",
    "- The answer is 4, implying four splits are needed. \n",
    "\n",
    "Similarly, for a billion elements, \n",
    "- \\( 2^{30} = 1,000,000,000 \\), \n",
    "- so only 30 splits are required to find a target in a billion-sized dataset. \n",
    "    - This efficiency makes logarithmic complexity highly valuable for large-scale data.\n",
    "\n",
    "To visualize this efficiency, consider how a linear search scales. \n",
    "\n",
    "- Searching a billion elements linearly would require a billion steps, \n",
    "- whereas a binary search would achieve the result in just 30 steps. \n",
    "- The power of logarithmic complexity becomes even more apparent as data sizes grow.\n",
    "- For a trillion elements (\\( 10^{12} \\)), \n",
    "    - just 40 steps suffice.\n",
    "\n",
    "In conclusion, logarithmic complexity is grounded in the principle of\n",
    "- reducing data size by half with each step. \n",
    "\n",
    "- The equation \\( 2^n = \\{data size} \\) or \n",
    "- its logarithmic equivalent \\( \\log_2(\\{data size}) = n \\)\n",
    "captures this concept mathematically. \n",
    "\n",
    "This efficiency is particularly evident in large datasets, \n",
    "- where it dramatically reduces the number of required operations compared to linear complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0907ad92",
   "metadata": {},
   "source": [
    "### Efficiency:\n",
    "Binary search achieves a time complexity of **O(log n)** \n",
    "- due to the consistent halving of the search space. \n",
    "\n",
    "Comparing it to other complexities:  \n",
    "- Constant time (**O(1)**) is ideal but rarely achievable in dynamic search scenarios.  \n",
    "- Linear time (**O(n)**) requires iterating through every element, \n",
    "    - which becomes inefficient for large datasets.\n",
    "\n",
    "### Visualization:\n",
    "When plotting complexities on a graph:\n",
    "\n",
    "- The x-axis represents **data size**, \n",
    "- and the y-axis represents **operations**.  \n",
    "- **O(1)** is constant and horizontal. **O(n)** is linear.  \n",
    "- **O(log n)** lies between **O(1)** and **O(n)**, \n",
    "    - showcasing its efficiency over larger datasets. \n",
    "- Algorithms like **merge sort** utilize **O(n log n)**, which we’ll cover later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9523d315",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
